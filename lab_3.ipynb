{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ahead-candidate",
   "metadata": {},
   "source": [
    "erefore we set the max_features parameter to 1500, which means that we want to use 1500 most occurring words as features for training our classifier.# Lab Week 6\n",
    "One issue in natural language processing is the identification of sentiment of texts, are they positive, negative or neutral to some concept. This dataset (https://github.com/thanhtut/info284_lab/blob/master/assignment1/twitter-airline-sentiment/Tweets.csv) has a collection of tweets about airlines with corresponding sentiments (negative, positive, neutral).\n",
    "\n",
    "The task :\n",
    "The task we want you to do is to run three machine learning models using Naïve Bayes on the tweet texts.\n",
    "\n",
    "1. Prepare the data for machine learning. Extract the relevant columns, do language preprocessing of the tweets, etc.\n",
    "2. Run a BernoulliNB to classify sentiment\n",
    "3. Run a MultiomialNB to classify sentiment\n",
    "4. Run a MultinomialNB to classify sentiment, but insteadof using a count vector you should use a TF-IDF-vector for each instance. What is TF-IDF? Read sources on information retrieval, and also look here: https://stackabuse.com/text-classification-with-python-and-scikit-learn\n",
    "\n",
    "How does these three approaches compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-essay",
   "metadata": {},
   "source": [
    "### What can we tell about the data?\n",
    "The dtypes vary, none of the data has nulls, we´ll double check anyways, mostly objects witch will be timeforamts and strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-morgan",
   "metadata": {},
   "source": [
    "We can analyse the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "transparent-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "median-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blank-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataframe is (14640, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dataframe is\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retired-framing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0      570306133677760513           neutral                        1.0000   \n",
       "1      570301130888122368          positive                        0.3486   \n",
       "2      570301083672813571           neutral                        0.6837   \n",
       "3      570301031407624196          negative                        1.0000   \n",
       "4      570300817074462722          negative                        1.0000   \n",
       "...                   ...               ...                           ...   \n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence         airline  \\\n",
       "0                         NaN                        NaN  Virgin America   \n",
       "1                         NaN                     0.0000  Virgin America   \n",
       "2                         NaN                        NaN  Virgin America   \n",
       "3                  Bad Flight                     0.7033  Virgin America   \n",
       "4                  Can't Tell                     1.0000  Virgin America   \n",
       "...                       ...                        ...             ...   \n",
       "14635                     NaN                     0.0000        American   \n",
       "14636  Customer Service Issue                     1.0000        American   \n",
       "14637                     NaN                        NaN        American   \n",
       "14638  Customer Service Issue                     0.6659        American   \n",
       "14639                     NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "0                        NaN          cairdin                 NaN   \n",
       "1                        NaN         jnardino                 NaN   \n",
       "2                        NaN       yvonnalynn                 NaN   \n",
       "3                        NaN         jnardino                 NaN   \n",
       "4                        NaN         jnardino                 NaN   \n",
       "...                      ...              ...                 ...   \n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "0                  0                @VirginAmerica What @dhepburn said.   \n",
       "1                  0  @VirginAmerica plus you've added commercials t...   \n",
       "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
       "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
       "...              ...                                                ...   \n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "...           ...                        ...            ...   \n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \n",
       "0      Eastern Time (US & Canada)  \n",
       "1      Pacific Time (US & Canada)  \n",
       "2      Central Time (US & Canada)  \n",
       "3      Pacific Time (US & Canada)  \n",
       "4      Pacific Time (US & Canada)  \n",
       "...                           ...  \n",
       "14635                         NaN  \n",
       "14636                         NaN  \n",
       "14637                         NaN  \n",
       "14638  Eastern Time (US & Canada)  \n",
       "14639                         NaN  \n",
       "\n",
       "[14640 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "encouraging-desperate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage null or na values in tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_id                         0.00\n",
       "airline_sentiment                0.00\n",
       "airline_sentiment_confidence     0.00\n",
       "negativereason                  37.31\n",
       "negativereason_confidence       28.13\n",
       "airline                          0.00\n",
       "airline_sentiment_gold          99.73\n",
       "name                             0.00\n",
       "negativereason_gold             99.78\n",
       "retweet_count                    0.00\n",
       "text                             0.00\n",
       "tweet_coord                     93.04\n",
       "tweet_created                    0.00\n",
       "tweet_location                  32.33\n",
       "user_timezone                   32.92\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percentage null or na values in tweets\")\n",
    "((df.isnull() | df.isna()).sum() * 100 / df.index.size).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sonic-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.duplicated().any())\n",
    "#print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demographic-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates()\n",
    "#print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-yahoo",
   "metadata": {},
   "source": [
    "### Choosing the necesarry columns:\n",
    "After removing duplications and looking for emtpy columns we can pick out the colums that should be used.\n",
    "`airline_sentiment_gold` , `negativereason_gold` and `tweet_coord` is all but empty so they will prove no help.\n",
    "`tweet_id`, `name` and `tweet_created` are rather unique so they will not give anything to the table. The confidence columns are also uneccesary. That leaves us with the remaining:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fancy-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.drop(columns = [\n",
    "    \"airline_sentiment\", \"airline_sentiment_confidence\", \"negativereason_confidence\",\n",
    "    \"tweet_id\", \"airline_sentiment_gold\", \"name\", \"negativereason_gold\",\n",
    "    \"tweet_coord\", \"tweet_created\", \"tweet_location\", \"user_timezone\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confidential-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   negativereason  9178 non-null   object\n",
      " 1   airline         14640 non-null  object\n",
      " 2   retweet_count   14640 non-null  int64 \n",
      " 3   text            14640 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 457.6+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "immediate-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets[\"text\"]\n",
    "y = df[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dutch-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recognized-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640,)\n",
      "(14640,)\n",
      "-------------\n",
      "(14640,)\n",
      "(14640,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "#y = y.to_numpy()\n",
    "#y = y.reshape(y.shape[0],1)\n",
    "print(\"-------------\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X = documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-deadline",
   "metadata": {},
   "source": [
    "Lets Vectorize the data using `CountVectorizer` the sklearn function converts a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secondary-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features = 2000, min_df=5, max_df=0.7, stop_words = stopwords.words(\"english\"))\n",
    "\n",
    "X = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-stockholm",
   "metadata": {},
   "source": [
    "Lets split the data into random train and test subsets and fit it to the `BernoulliNB()` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "precise-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "known-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB().fit(X_train, y_train)\n",
    "p_train = model.predict(X_train)\n",
    "p_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-lecture",
   "metadata": {},
   "source": [
    "Now let us calculate the accurasy and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pointed-medicare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8141165755919855\n",
      " Test Accuracy: 0.773224043715847\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_train, p_train)\n",
    "acc_test = accuracy_score(y_test, p_test)\n",
    "print(f\"Train Accuracy: {acc_train}\\n Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surprised-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "p_train = model.predict(X_train)\n",
    "p_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fantastic-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8103825136612022\n",
      " Test Accuracy: 0.771311475409836\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_train, p_train)\n",
    "acc_test = accuracy_score(y_test, p_test)\n",
    "print(f\"Train Accuracy: {acc_train}\\n Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-advice",
   "metadata": {},
   "source": [
    "> Finding TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "present-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidconverter = TfidfTransformer()\n",
    "X = tfidconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "corrected-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "absolute-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "p_train = model.predict(X_train)\n",
    "p_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hairy-fairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7826047358834244\n",
      " Test Accuracy: 0.7543715846994535\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_train, p_train)\n",
    "acc_test = accuracy_score(y_test, p_test)\n",
    "print(f\"Train Accuracy: {acc_train}\\n Test Accuracy: {acc_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
