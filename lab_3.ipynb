{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stuck-camera",
   "metadata": {},
   "source": [
    "# Lab Week 6\n",
    "One issue in natural language processing is the identification of sentiment of texts, are they positive, negative or neutral to some concept. This dataset (https://github.com/thanhtut/info284_lab/blob/master/assignment1/twitter-airline-sentiment/Tweets.csv) has a collection of tweets about airlines with corresponding sentiments (negative, positive, neutral).\n",
    "\n",
    "The task :\n",
    "The task we want you to do is to run three machine learning models using Naïve Bayes on the tweet texts.\n",
    "\n",
    "1. Prepare the data for machine learning. Extract the relevant columns, do language preprocessing of the tweets, etc.\n",
    "2. Run a BernoulliNB to classify sentiment\n",
    "3. Run a MultiomialNB to classify sentiment\n",
    "4. Run a MultinomialNB to classify sentiment, but insteadof using a count vector you should use a TF-IDF-vector for each instance. What is TF-IDF? Read sources on information retrieval, and also look here: https://stackabuse.com/text-classification-with-python-and-scikit-learn\n",
    "\n",
    "How does these three approaches compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-survival",
   "metadata": {},
   "source": [
    "### What can we tell about the data?\n",
    "The dtypes vary, none of the data has nulls, we´ll double check anyways, mostly objects witch will be timeforamts and strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-hunger",
   "metadata": {},
   "source": [
    "We can analyse the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flexible-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grand-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occupied-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataframe is (14640, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dataframe is\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "explicit-divide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage null or na values in tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_id                         0.00\n",
       "airline_sentiment                0.00\n",
       "airline_sentiment_confidence     0.00\n",
       "negativereason                  37.31\n",
       "negativereason_confidence       28.13\n",
       "airline                          0.00\n",
       "airline_sentiment_gold          99.73\n",
       "name                             0.00\n",
       "negativereason_gold             99.78\n",
       "retweet_count                    0.00\n",
       "text                             0.00\n",
       "tweet_coord                     93.04\n",
       "tweet_created                    0.00\n",
       "tweet_location                  32.33\n",
       "user_timezone                   32.92\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percentage null or na values in tweets\")\n",
    "((df.isnull() | df.isna()).sum() * 100 / df.index.size).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weird-needle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "14640\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().any())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heated-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14604\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-mambo",
   "metadata": {},
   "source": [
    "### Choosing the necesarry columns:\n",
    "After removing duplications and looking for emtpy columns we can pick out the colums that should be used.\n",
    "`airline_sentiment_gold` , `negativereason_gold` and `tweet_coord` is all but empty so they will prove no help.\n",
    "`tweet_id`, `name` and `tweet_created` are rather unique so they will not give anything to the table. The confidence columns are also uneccesary. That leaves us with the remaining:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "temporal-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.drop(columns = [\n",
    "    \"airline_sentiment\", \"airline_sentiment_confidence\", \"negativereason_confidence\",\n",
    "    \"tweet_id\", \"airline_sentiment_gold\", \"name\", \"negativereason_gold\",\n",
    "    \"tweet_coord\", \"tweet_created\", \"tweet_location\", \"user_timezone\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "meaning-horizontal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14604 entries, 0 to 14639\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   negativereason  9159 non-null   object\n",
      " 1   airline         14604 non-null  object\n",
      " 2   retweet_count   14604 non-null  int64 \n",
      " 3   text            14604 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 570.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "latter-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets\n",
    "y = df[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "latter-merchandise",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-b30baeaef180>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-b30baeaef180>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    y = np(y)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape\n",
    "y = np(y)\n",
    "y = y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-service",
   "metadata": {},
   "source": [
    "Lets Vectorize the data using `CountVectorizer` the sklearn function converts a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crude-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range = (1,6))\n",
    "\n",
    "X = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-driving",
   "metadata": {},
   "source": [
    "Lets split the data into random train and test subsets and fit it to the `BernoulliNB()` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "signed-olympus",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4, 14604]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-35bde8e9cefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/uni/machine-learning_284/.venv/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uni/machine-learning_284/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[1;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uni/machine-learning_284/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4, 14604]"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB().fit(X_train, y_train)\n",
    "p_train = model.predict(X_train)\n",
    "p_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-sustainability",
   "metadata": {},
   "source": [
    "Now let us calculate the accurasy and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_train, p_train)\n",
    "acc_test = accuracy_score(y_test, p_test)\n",
    "print(f\"Train Accuracy: {acc_train}\\n Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-ceiling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
