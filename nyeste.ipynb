{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Group Exam INFO284 V21*\n",
    "\n",
    "# *SALE PRICE PREDICTION NYC SALES*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Members: Didrik Nettelhorst Krog, Jonas Bech Holtan, Gunnar Hole Gjengedal, Snorre Alvsvåg**\n",
    "\n",
    "\n",
    "## Introduction\n",
    "### The task\n",
    "You are supposed to build at least five machine learning models from these data to predict or\n",
    "classify one relevant target feature for new data points. You can choose target feature yourself, but\n",
    "sales price is perhaps the most suitable. You may also reduce the number of data points somewhat\n",
    "by focusing on only specific meaningful parts of the data. Or perhaps you will try dimension\n",
    "reduction.\n",
    "\n",
    "### Our Approach\n",
    "This task can be split into four parts:\n",
    "- **Part 1** Data Exploration\n",
    "- **Part 2** Data Cleaning\n",
    "- **Part 3** Model Building\n",
    "- **Part 4** Presentation and Analysation\n",
    "\n",
    "We will explore our data by conducting a *Exploratory Data Analysis*. Here we will look at the data and make note of important features, non important features, and in general inform ourselves with the data. In part 2, we will generally clean the data, remove empty or unique columns and so on. In part 3 we will start to fit the models, we will also change the data in preparation of each model. And at last we will present the results, and analyze the different approaches we took with out models.\n",
    "\n",
    "We have chosen these supervised machine learning algorythms:\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "- Gradient Boosting Regressor\n",
    "\n",
    "- Linear Reggresion \n",
    "- Ridge Reggression\n",
    "- Lasso Regression\n",
    "- Neural Network (MLPRegressor)\n",
    "\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "- Gaussian Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "\n",
    "We have chosen one unsupervised machine learning algorythms:\n",
    "- Clustering\n",
    "\n",
    "Moving forward we will start by learing about our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data and Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('nyc-rolling-sales.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVR, SVR, NuSVR \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presenting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see above, we have several different types of data that are both categorical and continous. Before creating the models necassary for this assignment, we need to convert them to the appropriate types. \n",
    "\n",
    "*The countinous features being :*\n",
    "\n",
    "- SALE PRICE\n",
    "    - Depending on which models we are using, 'SALE PRICE' will be continous and categorical. This is because this feature will       serve as our target feature and must be the apprioriate type for the specific models. \n",
    "- LAND SQUARE FEET\n",
    "    - The land area of the property listed in square feet\n",
    "    \n",
    "- GROSS SQUARE FEET \n",
    "    - The total area of all the floors of a building as measured from the exterior surfaces of the outside    walls of the\n",
    "      building, including the land area and space within any building or structure on the property.\n",
    "      \n",
    "- SALE DATE (Which will be converted to MONTH SOLD and YEAR SOLD)\n",
    "    - This is going to be categorical, 'MONTH SOLD' is the month of when the property is sold. 'YEAR SOLD' is the year the\n",
    "      property was sold.\n",
    "    \n",
    "- COMMERCIAL UNITS\n",
    "    - The number of commercial units at the listed property.\n",
    "    \n",
    "- RESIDENTIAL UNITS \n",
    "    - The number of residential units at the listed property.\n",
    "    \n",
    "- TOTAL UNITS\n",
    "    - The total number of units at the listed property.\n",
    "    \n",
    "- BLOCK\n",
    "    - Because there are more than 11k unique blocks in the dataset, it doesn't make sense to define it as a categorical \n",
    "      variable\n",
    "      \n",
    "- LOT\n",
    "    - The same reason as 'BLOCK' feature\n",
    "\n",
    "*The categorical features being :*\n",
    "\n",
    "- BOROUGH\n",
    "    - A digit code for the borough the property is located in; in order these are Manhattan (1), Bronx (2), Brooklyn (3),\n",
    "      Queens (4), and Staten Island (5).\n",
    "      \n",
    "- NEIGHBORHOOD\n",
    "    - Names of the neighboohoods that work better as a categorical feature than a numerical\n",
    "    \n",
    "- ZIP CODE\n",
    "    - The property’s postal code.\n",
    "    \n",
    "- BUILDING CLASS CATEGORY\n",
    "    - Easier identifiable categories of the possible types of buldings \n",
    "    \n",
    "- TAX CLASS AT PRESENT\n",
    "    - Every property in the city is assigned to one of four tax classes, based on the use of the property.\n",
    "    \n",
    "- BUILDING CLASS AT PRESENT\n",
    "    - The Building Classification is used to describe a property’s constructive use. The first position of the Building Class\n",
    "      is a letter that is used to describe a general class of properties. The second position, a number, adds more specific\n",
    "      information about the property’s use or construction style.\n",
    "      \n",
    "- YEAR BUILT\n",
    "    - Year the structure on the property was built.\n",
    "    \n",
    "- TAX CLASS AT TIME OF SALE\n",
    "    - See 'TAX CLASS AT PRESENT'\n",
    "    \n",
    "- BUILDING CLASS AT TIME OF SALE\n",
    "    - See 'BUILDING CLASS AT PRSENT'\n",
    "\n",
    "*The features we defintely dont need is :*\n",
    "\n",
    "- Unnamed: 0 \n",
    "    - Dropping it because it just looks like an iterator\n",
    "    \n",
    "- EASE-MENT \n",
    "    - Because it only contains NaN values\n",
    "    \n",
    "- APARTMENT NUMBER \n",
    "    - The number of the apartment is not relevant for sale price\n",
    "    \n",
    "- ADDRESS\n",
    "    - Address is just listed as names and we feel that it won't affect the sales price prediction at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Overall Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next session is about converting, reducing and cleaning the features we are going to use in our models. The type of cleaning we are doing here is for the Regression and Tree Models we are using later in the code. Since we also are using Gaussian and Bernoulli Naive Bayes Models, there will be a second data cleaning and pre-processing later in the code after the codes mentioned above.**\n",
    "\n",
    "**Either way, the most important in this data cleaning here is to convert the necessary features to more appropriate types and remove most of the outliners. We will end up with some outliners either way because of the complexity of our dataset. However, with our Unsupervised Machine Learning Model, k-means, the models is expected to perform good even with many outliners.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features we definetly dont need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['EASE-MENT', 'Unnamed: 0', 'APARTMENT NUMBER', \"ADDRESS\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.duplicated(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(df.columns, keep='last')\n",
    "sum(df.duplicated(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are any null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any(), df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing empty or '-' values with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(' ', np.nan)\n",
    "df = df.replace(' -  ', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of where the NaN values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage null or na values in Dataset\\n-------------------------------------\")\n",
    "((df.isnull() | df.isna()).sum() * 100 / df.index.size).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing some of the features\n",
    "\n",
    "Before further inspections, we decided to remove some more of the features because we dont think they are necessary. These are:\n",
    "\n",
    "- \"ZIP CODE\" : \n",
    "\n",
    "- \"BLOCK\" :\n",
    "\n",
    "- \"LOT\" : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ZIP CODE', 'BLOCK', 'LOT'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Sale Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all the NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['SALE PRICE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting 'SALE PRICE' to a more appropriate type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SALE PRICE\"] = df[\"SALE PRICE\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a diagram to get a clearer view of how 'SALE PRICE' is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['SALE PRICE'], kde=True, bins=50, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distribution above, there are a lot of outliners around 2000000 and above. At the same time, the majority of the values are close to 0. The most optimal place to is between 3.000.000 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['SALE PRICE'] < 10000) | (df['SALE PRICE'] > 3000000)]['SALE PRICE'].count() /len(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that 22% of the values are either greater than 3.000.000 or less than 10.000. We remove this to get a better distribution of 'SALE PRICE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[(df['SALE PRICE'] > 10000) & (df['SALE PRICE']<3000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SALE PRICE'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['SALE PRICE'], kde=True, bins=50, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 'SALE PRICE' feature, our target feature, is now more appropriated distributed for our models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Square Feet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of missing values in these features.\n",
    "We can either fill them up with the mean average or remove them like this:\n",
    "\n",
    "For the time being, lets remove this missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['LAND SQUARE FEET'])\n",
    "df = df.dropna(subset=['GROSS SQUARE FEET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the features to a more appropriate type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LAND SQUARE FEET'] = df['LAND SQUARE FEET'].astype(float)\n",
    "df['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LAND SQUARE FEET'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GROSS SQUARE FEET'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of 0 values here, and means that the properties in this category has no square feet. we will remove these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['LAND SQUARE FEET'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['GROSS SQUARE FEET'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['LAND SQUARE FEET'], kde=True, bins=50, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['GROSS SQUARE FEET'], kde=True, bins=50, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see from the diagrams and value_counts(), there are a lot of outliners in these features. \n",
    "These can be cleaned by just reducing the number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['LAND SQUARE FEET'] < 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['GROSS SQUARE FEET'] < 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['LAND SQUARE FEET'], kde=True, bins=50, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['GROSS SQUARE FEET'], kde=True, bins=50, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Sale Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TimeStamp types are hard to work with. Therefore, we make the 'SALE DATE' feature to two distinct features 'YEAR SOLD' and\n",
    "'MONTH SOLD'. These two features can help us later in further analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SALE DATE'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a list with alle the different months "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting Sale Date into to features, one for Month Sold and one for Year Sold. This is first for making a TimeStamp feature easier to use, and for calculating which month or year had the highest sales rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YEAR SOLD'] = [int(n[0:4]) for n in df['SALE DATE']]\n",
    "df['MONTH SOLD'] = [int(n[5:7]) for n in df['SALE DATE']]\n",
    "df = df.drop(['SALE DATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['YEAR SOLD'], bins=2, color='c')\n",
    "counts_per_year = [sum(df['YEAR SOLD'] == 2016), sum(df['YEAR SOLD'] == 2017)]\n",
    "sns.barplot([2016, 2017], counts_per_year).set_title('Number of properties sold by year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first diagram shows that there were significant more houses sold in 2017 than in 2016, showing an increase that may indicate an even bigger increase in a predictive model for 2018\n",
    "\n",
    "Code retrieved from: https://jshams.github.io/NYC-real-estate-analysis/new_york_real_estate.slides.html#/5/2 and https://jshams.github.io/NYC-real-estate-analysis/new_york_real_estate.slides.html#/6/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming to more appriopriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MONTH SOLD'] = df['MONTH SOLD'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YEAR SOLD'] = df['YEAR SOLD'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Tax Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all the NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['TAX CLASS AT PRESENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT TIME OF SALE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT PRESENT'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tax classes, as mentioned earlier, should be categorical classes. Therefore, it is more appropriate to replace the values to more informative values as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT TIME OF SALE'] = df['TAX CLASS AT TIME OF SALE'].replace({1:'Class_1',\n",
    "                                                                           2:'Class_2',\n",
    "                                                                           4:'Class_4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT PRESENT'] = df['TAX CLASS AT PRESENT'].replace({'1':'Class_1',\n",
    "                                                                 '1A':'Class_1',\n",
    "                                                                 '1B':'Class_1',\n",
    "                                                                 '1C':'Class_1',\n",
    "                                                                 '2':'Class_2',\n",
    "                                                                 '2A':'Class_2',\n",
    "                                                                 '2B':'Class_2',\n",
    "                                                                 '2C':'Class_2',\n",
    "                                                                 '3':'Class_3',\n",
    "                                                                 '4':'Class_4'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the features to more appropiate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT TIME OF SALE'] = df['TAX CLASS AT TIME OF SALE'].astype('category')\n",
    "df['TAX CLASS AT PRESENT'] = df['TAX CLASS AT PRESENT'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT PRESENT'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT TIME OF SALE'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These pie charts show that more the 3/4 of all the values in the features belong to class_1.** \n",
    "\n",
    "- Class 1: Includes most residential property of up to three units (such as one-, two-, and three-family homes and small stores   or offices with one or two attached apartments), vacant land that is zoned for residential use, and most condominiums that     are not more than three stories.\n",
    "- Class 2: Includes all other property that is primarily residential, such as cooperatives and condominiums.\n",
    "- Class 4: Includes all other properties not included in class 1,2, and 3, such as offices, factories, warehouses, garage         buildings, etc. \n",
    "\n",
    "**We have chosen to not remove class_2 and class_4 because we want to focus on all of the properties instead of only focusing on residential properties.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Buliding Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the features to more appropiate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS AT TIME OF SALE'] = df['BUILDING CLASS AT TIME OF SALE'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'BUILDING CLASS AT TIME OF SALE' and 'BUILDING CLASS CATEGORY' is significant the same. We remove one of them to prevent having to many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['BUILDING CLASS AT PRESENT'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_types = {'01 ONE FAMILY DWELLINGS': 12390,\n",
    " '02 TWO FAMILY DWELLINGS': 9590,\n",
    " '03 THREE FAMILY DWELLINGS': 2239,\n",
    " '07 RENTALS - WALKUP APARTMENTS': 1177,\n",
    " 'Others' : 1045}                         # 'Others' here is the sum of fifth - 28th value given above\n",
    "\n",
    "labels = property_types.keys()\n",
    "sizes = property_types.values()\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#be3cb2']\n",
    "plt.pie(sizes, colors = colors, labels=labels, autopct='%1.2f%%',startangle=35, pctdistance=0.75, explode = tuple([0.05] * 5))\n",
    "centre_circle = plt.Circle((0,0),0.50,fc='#ffffff')\n",
    "plt.gcf().gca().add_artist(centre_circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value_count() and Pie Chart above, we can observe that the first four values stands out the most and the rest should rather be one value to remove a lot of outliners. This is because the others value is a lower percentage than the fourth value. Code retrieved from: https://jshams.github.io/NYC-real-estate-analysis/new_york_real_estate.slides.html#/8\n",
    "\n",
    "We tried to make a new feature called others with the fifth all the way down to the 20th value, but it seems it is not possible to combine all of them. Either way, we chose to keep the feature anyway. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to a more appropriate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS CATEGORY'] = df['BUILDING CLASS CATEGORY'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Borough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change borough index to borough real name in New York City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BOROUGH'][df['BOROUGH'] == 1] = 'Manhattan'\n",
    "df['BOROUGH'][df['BOROUGH'] == 2] = 'Bronx'\n",
    "df['BOROUGH'][df['BOROUGH'] == 3] = 'Brooklyn'\n",
    "df['BOROUGH'][df['BOROUGH'] == 4] = 'Queens'\n",
    "df['BOROUGH'][df['BOROUGH'] == 5] = 'Staten Island'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting 'BOROUGH' to a more appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BOROUGH'] = df['BOROUGH'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('BOROUGH',data=df,palette='Set2')\n",
    "plt.title('Sales per Borough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(y = 'BOROUGH', x = 'SALE PRICE', data = df )\n",
    "plt.title('Box plots for SALE PRICE on each BOROUGH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These diagrams above shows a how big difference of property sold and outliners there exists in this features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Year Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['YEAR BUILT']!=0]\n",
    "sns.distplot(df['YEAR BUILT'], bins=50, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As its possible to se from the graph above, the feature will work best if the values from 1800 - 1900 are removed. With this, we removed some of the possible outliners in our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[(df['YEAR BUILT'] > 1880)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the feature to a more appropriate type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YEAR BUILT'] = df['YEAR BUILT'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning \"... Units\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping 'TOTAL UNITS' because its only the sum of 'RESIDENTIAL UNITS' and 'COMMERCIAL UNITS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['TOTAL UNITS'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('RESIDENTIAL UNITS',data=df,palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('COMMERCIAL UNITS',data=df,palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[(df['RESIDENTIAL UNITS'] < 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[(df['COMMERCIAL UNITS'] < 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 'Units' features have a really skewed distribution. To avoid to many outliners, We only include the values with the most counts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the feature to a more appropriate type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NEIGHBORHOOD'] = df['NEIGHBORHOOD'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By now, all the necessary features have been cleaned from most of outliners and converted to the preferred types needed for the algorithms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing - Making Categorical Data more Appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since most of our models prefer continous features, and our dataset contains an significant amount categorical data, we have to encode our categorical data so we can actually use the features in our models. For this we will use One-Hot-Encoding and Label Encoding, depending on how many unique values there exists in the features.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BOROUGH'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT PRESENT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT TIME OF SALE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YEAR SOLD'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These categorical features above have few unique values which makes One-Hot-Encoding the most appropriate way to make the features continious**\n",
    "\n",
    "**We make a list of all these features for an easier overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_features = ['BOROUGH', 'TAX CLASS AT PRESENT', 'TAX CLASS AT TIME OF SALE', 'BUILDING CLASS CATEGORY', 'YEAR SOLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(df[one_hot_features])\n",
    "\n",
    "df = df.drop(one_hot_features, axis = 1)\n",
    "\n",
    "df = pd.concat([df, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rest of the categorical features are a little bit different. There are hundres of unique features which makes Label Encoding more appropriate than One-Hot_encoding. This is for preventing a higher dimensionality and ending up with many more features necassary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NEIGHBORHOOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['NEIGHBORHOOD'] = label_encoder.fit_transform(df['NEIGHBORHOOD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS AT TIME OF SALE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_2 = LabelEncoder()\n",
    "\n",
    "df['BUILDING CLASS AT TIME OF SALE'] = label_encoder_2.fit_transform(df['BUILDING CLASS AT TIME OF SALE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YEAR BUILT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_3 = LabelEncoder()\n",
    "\n",
    "df['YEAR BUILT'] = label_encoder_3.fit_transform(df['YEAR BUILT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MONTH SOLD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_4 = LabelEncoder()\n",
    "\n",
    "df['MONTH SOLD'] = label_encoder_4.fit_transform(df['MONTH SOLD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To imporve the overall performance of the models, we decided to use k-means method to make clusters to see if there exists any logical ways to group the data in a feature space**\n",
    "\n",
    "We start by just making a scattermap overview with all the features."
   ]
  },
  {
   "source": [
    "scatter_df = df.drop(['SALE PRICE'], axis = 1)\n",
    "\n",
    "for col in scatter_df.columns: \n",
    "    plt.scatter(scatter_df[col], df['SALE PRICE']) \n",
    "    plt.ylabel('Sale price') \n",
    "    plt.xlabel(col) \n",
    "    plt.show()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scattermaps shows above, the different features are scattered very different from each other because of the categorical features. The continous features show a vide fully scattered maps and the categorical features are scattered like lines. This will affect our k-means cluster model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using k-means clustering, we need to define a k. k is defined as how many clusters we want in our unsupervised model. Thats why we use the elbow method to get the most optimal k.\n",
    "\n",
    "The code used below for our Unsupervised Learning Model is retrieved from: https://predictivehacks.com/k-means-elbow-method-code-for-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(df)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-') \n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model above shows us that the most optimal k is two. We only need two clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanModel = KMeans(n_clusters=2)\n",
    "kmeanModel.fit(df)\n",
    "\n",
    "df['k_means'] = kmeanModel.predict(df)\n",
    "df['SALE PRICE'] = df['SALE PRICE']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24,12))\n",
    "axes[0].scatter(df['k_means'], df['SALE PRICE'], c=df['SALE PRICE'])\n",
    "axes[1].scatter(df['k_means'], df['SALE PRICE'], c=df['k_means'], cmap=plt.cm.Set1)\n",
    "axes[0].set_title('Actual', fontsize=18)\n",
    "axes[1].set_title('K_Means', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the models above show, the k-means clustering result is questionable. They show us two straigth lines in two very different intervals and does not really tell us much about how the groupings of the different features are. This might be because of the high complexity of our data or because of the how the categorical features are placed in a feature space, as the scatter maps above shows.**\n",
    "\n",
    "**However, as we have seen in different tests of the machine learning models, the visualization of the Unsupervised Learning Model might not be good at all, but the models perform way better with the k-means feature than without. This means that the k-means clustering is of value and there actually are groupings of the features in a feature space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df_columns = minmax_df.columns\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_df = minmax_scaler.fit_transform(minmax_df)\n",
    "\n",
    "minmax_df = pd.DataFrame(minmax_df)\n",
    "minmax_df.columns = minmax_df_columns\n",
    "\n",
    "minmax_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to make the most accurate models possible, we should use Principal Component Ananlysis (PCA). To do this, we have to use a Standard Scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df_columns = standard_df.columns\n",
    "scaler = StandardScaler()\n",
    "standard_df = scaler.fit_transform(standard_df)\n",
    "\n",
    "standard_df = pd.DataFrame(standard_df)\n",
    "standard_df.columns = standard_df_columns\n",
    "\n",
    "standard_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have used our data in the Unsupervised Model and defined our necessary scaling methods, we are now ready to create our Machine Learning Models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------- NYC SALES PREDICTION -----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we run every Model, we will perform a Grid Search to see what the best parameters for each model is**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these Tree Models don't need any scaling, we have chosen to not use any because of the scores without scaling are better.\n",
    "\n",
    "We still need to split the data set in a train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['SALE PRICE']\n",
    "x = df.drop(['SALE PRICE'], axis = 1)\n",
    "\n",
    "x_train, x_test , y_train, y_test = train_test_split(x , y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt = DecisionTreeRegressor().fit(x_train, y_train)\n",
    "\n",
    "#param_grid = {'max_depth' : [1,5,10,15,20,50,100]}\n",
    "#grid_dt = GridSearchCV(DecisionTreeRegressor() , param_grid = param_grid)\n",
    "#grid_dt.fit(x_train , y_train)\n",
    "\n",
    "#print('Grid search with the best accuracy: \\n------------------------------------')\n",
    "#print('Best parameters: ', grid_dt.best_params_) \n",
    "#print('Best cross validation score(Accuracy): {:.3f}'.format(grid_dt.best_score_))\n",
    "\n",
    "#print('Test set score: {:.3f}'.format(grid_dt.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'max_depth: 10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(max_depth=10).fit(x_train, y_train)\n",
    "\n",
    "print(\"Decision Tree Classifier Model\")\n",
    "print(\"------------------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(dt.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(dt.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForestRegressor().fit(x_train, y_train)\n",
    "\n",
    "#param_grid = {'max_depth' : [1,5,10,15,20,50,100]}\n",
    "#grid_rf = GridSearchCV(RandomForestRegressor() , param_grid = param_grid)\n",
    "#grid_rf.fit(x_train , y_train)\n",
    "\n",
    "#print('Grid search with the best accuracy: \\n------------------------------------')\n",
    "#print('Best parameters: ', grid_rf.best_params_) \n",
    "#print('Best cross validation score(Accuracy): {:.3f}'.format(grid_rf.best_score_))\n",
    "\n",
    "#print('Test set score: {:.3f}'.format(grid_rf.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'max_depth: 10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_depth=10).fit(x_train, y_train)\n",
    "\n",
    "print(\"Random Forest Regressor Model\")\n",
    "print(\"-----------------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(rf.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(rf.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbr = GradientBoostingRegressor(n_estimators = 500)\n",
    "#param_grid = {'learning_rate' : [0.0001,0.001,0.01,1,0.05,0.1, 0.15, 0.5]}\n",
    "#grid_gbr = GridSearchCV(gbr , param_grid , n_jobs = -1)\n",
    "#grid_gbr.fit(x_train , y_train)\n",
    "\n",
    "#print('Grid search with the best accuracy:')\n",
    "#print('-----------------------------------')\n",
    "#print('Best parameters: ', grid_gbr.best_params_) \n",
    "#print('Best cross validation score(Accuracy): {:.5f}'.format(grid_gbr.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'learning_rate: 0.15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators = 500, learning_rate=0.15)\n",
    "gbr.fit(x_train, y_train)\n",
    "\n",
    "print(\"Gradient Boosting Regressor Model\")\n",
    "print(\"---------------------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(gbr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(gbr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These Tree Models aboves gives extremely good scores. Its no suprise that the Random Forest Model has a higher score than the Decision Tree Model, and the Gradient Booster Regressor has a higher score than the Random Forest Model. Random Forest has 'more brances' than the Decision Tree Model and gives better possiblities for the model to choose, while the Gradient Boosting Regressor is able to find the most suited 'branch' out of all the 'branches' and therefore has the best score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = standard_df['SALE PRICE']\n",
    "x = standard_df.drop(['SALE PRICE'], axis = 1)\n",
    "\n",
    "print(\"The shape of the target set:\", y.shape)\n",
    "print(\"The shape of the dataset:\", x.shape)\n",
    "\n",
    "x_train, x_test , y_train, y_test = train_test_split(x , y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(x_train , y_train)\n",
    "\n",
    "print(\"Normal Linear Regression Model\")\n",
    "print(\"------------------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(lr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge = Ridge().fit(x_train, y_train)\n",
    "\n",
    "#alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "#param_grid = dict(alpha=alpha)\n",
    "\n",
    "#grid = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2')\n",
    "#grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "#print('Best Score: ', grid_result.best_score_)\n",
    "#print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'alpha: 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=100).fit(x_train, y_train)\n",
    "\n",
    "print(\"Ridge Regression Model\")\n",
    "print(\"----------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(ridge.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(ridge.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso = Lasso().fit(x_train, y_train)\n",
    "\n",
    "#alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "#param_grid = dict(alpha=alpha)\n",
    "\n",
    "#grid = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2')\n",
    "#grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "#print('Best Score: ', grid_result.best_score_)\n",
    "#print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'alpha: 0.001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.01).fit(x_train, y_train)\n",
    "\n",
    "print(\"Lasso Regression Model\")\n",
    "print(\"----------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(lasso.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(lasso.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn = MLPRegressor()\n",
    "#param_grid = {\"hidden_layer_sizes\": [(1,),(10,),(25),(50,),(60,),(70,),(80,),(90,)], \n",
    "#              \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"], \n",
    "#              \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "#grid_nn = GridSearchCV(nn, param_grid , n_jobs = -1)\n",
    "#grid_nn.fit(x_train , y_train)\n",
    "\n",
    "#print('Grid search with the best accuracy:')\n",
    "#print('-----------------------------------')\n",
    "#print('Best parameters: ', grid_nn.best_params_) \n",
    "#print('Best cross validation score(Accuracy): {:.5f}'.format(grid_nn.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'alpha: 0.01' and 'hidden_layers_sizes: (80,)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Neural Network Regressor Model\n------------------------------\n\nTraining set score: 0.02410\nTest set score: -0.01640\n"
     ]
    }
   ],
   "source": [
    "nn = MLPRegressor(hidden_layer_sizes= [80,45], activation='relu', alpha=0.1)\n",
    "nn.fit(x_train, y_train)\n",
    "\n",
    "print(\"Neural Network Regressor Model\")\n",
    "print(\"------------------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(nn.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(nn.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Regressions above gives us a score that is overall not that bad. \n",
    "Linear Regression scores significantly bad, but that is expected because of the features are not linear and then the linear line from this model will not be able to make a straigth line.**\n",
    "\n",
    "**Ridge and Lasso scores overall better than the linear because of the models being more appriopriate for complex data, and the grid search gives us the most optimal alpha.**\n",
    "\n",
    "**Lastly, the Neural Network Model scores best. This is expected because of Neaural Network being the most suited for complex and big data. The reason being the parameter with hidden layers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian and Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Machine Learning Models is a little bit different than the others. Since this Model is more about classification, we have to, as mentioned before, convert our target feature 'SALE PRICE' into a categorical type. Since the date pre-processing is different than the Models above, we have to do a new pre-processing. This will be based on the pre-processing we did before.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catedf = df\n",
    "catedf3 = catedf[(catedf['SALE PRICE'] >= 1400000)]\n",
    "catedf2 = catedf[(catedf['SALE PRICE'] >= 850000) & (catedf['SALE PRICE'] < 1400000)]\n",
    "catedf1 = catedf[(catedf['SALE PRICE'] >= 400000) & (catedf['SALE PRICE'] < 850000)]\n",
    "catedf0 = catedf[(catedf['SALE PRICE'] < 400000)]\n",
    "names_list = [catedf0, catedf1 , catedf2 , catedf3]\n",
    "final_list = []\n",
    "new_df = []\n",
    "for i in range(4):\n",
    "    numbers_list = [i]*len(names_list[i])\n",
    "\n",
    "    catedf = names_list[i]\n",
    "    catedf['CATEGORICAL PRICE'] = numbers_list\n",
    "    new_df.append(catedf)\n",
    "catedf = new_df[0].append([new_df[1],new_df[2],new_df[3]])\n",
    "catedf = catedf.drop(['SALE PRICE'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing dataframe into one contaning the categorical variables, and one containing the continous. \n",
    "However some of the categorical data has not been categorized using one-hot-encoding. Therefore these variables are not binary, and can not be used for categorical NaiveBayes, will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df = catedf.drop(['GROSS SQUARE FEET' , 'LAND SQUARE FEET' , 'COMMERCIAL UNITS' , 'RESIDENTIAL UNITS', 'NEIGHBORHOOD' ,\n",
    "                              'BUILDING CLASS AT TIME OF SALE', 'YEAR BUILT' , 'MONTH SOLD'] ,  axis = 1)\n",
    "\n",
    "continous_df = catedf[['GROSS SQUARE FEET' , 'LAND SQUARE FEET' , 'CATEGORICAL PRICE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continous data now needs to be given a normal distribution, in order to make GaussianNB valid. The instances that equal 0 need to be removed as it ruins the normal distribution. The variables commercial units and residential units will be dropped, as they can not be normalized. In order to make columns logarithmic we used code from: https://www.datasciencemadesimple.com/log-natural-logarithmic-value-column-pandas-python-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_df = continous_df[(continous_df['GROSS SQUARE FEET'] > 0)]\n",
    "continous_df = continous_df[(continous_df['LAND SQUARE FEET'] > 0)]\n",
    "\n",
    "\n",
    "continous_df['LOG GROSS SQUARE FEET'] = np.log(continous_df['GROSS SQUARE FEET'])\n",
    "continous_df['LOG LAND SQUARE FEET'] = np.log(continous_df['LAND SQUARE FEET'])\n",
    "continous_df.drop(['GROSS SQUARE FEET' , 'LAND SQUARE FEET' ], axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = continous_df.drop(['CATEGORICAL PRICE'] , axis = 1)\n",
    "y = continous_df['CATEGORICAL PRICE']\n",
    "\n",
    "x_train, x_test , y_train , y_test = train_test_split(x,y , random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "print(\"Gaussian Naive Bayes Model\")\n",
    "print(\"--------------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(gnb.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(gnb.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = categorical_df.drop(['CATEGORICAL PRICE'] , axis = 1)\n",
    "y = categorical_df['CATEGORICAL PRICE']\n",
    "\n",
    "\n",
    "x_train, x_test , y_train , y_test = train_test_split(x, y, random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = {'alpha' : [0.000001 , 0.00001 , 0.0001 , 0.001 , 0.01 , 0.1 , 1 ,10]}\n",
    "#grid_bernouli = GridSearchCV(BernoulliNB() , param_grid = param_grid)\n",
    "#grid_bernouli.fit(x_train , y_train)\n",
    "\n",
    "#print('Grid search with the best accuracy: \\n------------------------------------')\n",
    "#print('Best parameters: ', grid_bernouli.best_params_) \n",
    "#print('Best cross validation score(Accuracy): {:.3f}'.format(grid_bernouli.best_score_))\n",
    "\n",
    "#print('Test set score: {:.3f}'.format(grid_bernouli.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'alpha: 0.01' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = BernoulliNB('alpha'== 0.01)\n",
    "bernoulli.fit(x_train , y_train)\n",
    "\n",
    "print(\"Bernoulli Naive Bayes Model\")\n",
    "print(\"---------------------------\")\n",
    "print()\n",
    "print(\"Training set score: {:.5f}\".format(bernoulli.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.5f}\".format(bernoulli.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted accuracy if the model only guesses the category with most appearances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(15768/33865)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get the best possible score, we will now try combining the categorical and continous variables into one model. First by binning the continous variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_df['BINNED LOG LAND SQUARE FEET'] = pd.qcut(continous_df['LOG LAND SQUARE FEET'] ,\n",
    "                                                  5 )\n",
    "continous_df['BINNED LOG GROSS SQUARE FEET'] = pd.qcut(continous_df['LOG GROSS SQUARE FEET'] ,\n",
    "                                                 5)\n",
    "\n",
    "one_hot_land = pd.get_dummies(continous_df['BINNED LOG LAND SQUARE FEET'])\n",
    "one_hot_gross = pd.get_dummies(continous_df['BINNED LOG GROSS SQUARE FEET'])\n",
    "\n",
    "categorical_df = pd.concat([categorical_df , one_hot_land , one_hot_gross],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = categorical_df.drop(['CATEGORICAL PRICE'] , axis = 1)\n",
    "y = categorical_df['CATEGORICAL PRICE']\n",
    "\n",
    "\n",
    "x_train, x_test , y_train , y_test = train_test_split(x,y , random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = {'alpha' : [0.000001 , 0.00001 , 0.0001 , 0.001 , 0.01 , 0.1 , 1 ,10]}\n",
    "#grid_bernouli = GridSearchCV(BernoulliNB() , param_grid = param_grid)\n",
    "#grid_bernouli.fit(x_train , y_train)\n",
    "\n",
    "#print('Grid search with the best accuracy: \\n------------------------------------')\n",
    "#print('Best parameters: ', grid_bernouli.best_params_) \n",
    "#print('Best cross validation score(Accuracy): {:.3f}'.format(grid_bernouli.best_score_))\n",
    "\n",
    "#print('Test set score: {:.3f}'.format(grid_bernouli.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch above gave us the paramter 'alpha: 0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = BernoulliNB('alpha' == 0.1)\n",
    "bernoulli.fit(x_train , y_train)\n",
    "\n",
    "\n",
    "print('Training set score: {:.5f}'.format(bernoulli.score(x_train , y_train)))\n",
    "print('Test set score: {:.5f}'. format(bernoulli.score(x_test , y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs better than when only using the continous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will now try another method for combining continous and categorical variables. The approach has been found on: \n",
    "https://towardsdatascience.com/naive-bayes-classifier-how-to-successfully-use-it-in-python-ecf76a995069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([categorical_df.drop(['CATEGORICAL PRICE'] ,axis = 1), continous_df[['LOG GROSS SQUARE FEET' , 'LOG LAND SQUARE FEET']]], axis = 1)\n",
    "y = categorical_df['CATEGORICAL PRICE']\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , random_state = 1)\n",
    "\n",
    "              \n",
    "\n",
    "gaussian_model = GaussianNB()\n",
    "clf_g = gaussian_model.fit(X_train[['LOG GROSS SQUARE FEET' , 'LOG LAND SQUARE FEET']] , y_train)\n",
    "\n",
    "categorical_model = BernoulliNB()\n",
    "clf_c = categorical_model.fit(X_train.drop(['LOG GROSS SQUARE FEET' , 'LOG LAND SQUARE FEET'] , axis = 1) \n",
    "                              , y_train)\n",
    "\n",
    "\n",
    "g_train_probs = gaussian_model.predict_proba(X_train[['LOG GROSS SQUARE FEET' , 'LOG LAND SQUARE FEET']])\n",
    "c_train_probs = categorical_model.predict_proba(X_train.drop(['LOG GROSS SQUARE FEET' , 'LOG LAND SQUARE FEET'], axis = 1 ))\n",
    " \n",
    "g_test_probs = gaussian_model.predict_proba(X_test[['LOG GROSS SQUARE FEET' , 'LOG LAND SQUARE FEET']])                                                 \n",
    "c_test_probs = categorical_model.predict_proba(X_test.drop(['LOG GROSS SQUARE FEET' , 'LOG LAND SQUARE FEET'], axis = 1 ))\n",
    "\n",
    "\n",
    "X_new_train = np.c_[(g_train_probs[:,1], c_train_probs[:,1])] # Train\n",
    "X_new_test = np.c_[(g_test_probs[:,1], c_test_probs[:,1])] # Test\n",
    "\n",
    "combined_gaussian = GaussianNB()\n",
    "combined_gaussian.fit(X_new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined Naive Bayes Model\")\n",
    "print(\"--------------------------\")\n",
    "print()\n",
    "print('Training set score: {:.5f}'.format(combined_gaussian.score(X_new_train, y_train)))\n",
    "print('Test set score: {:.5f}'.format(combined_gaussian.score(X_new_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seems like the Naive Bayes models do not perform better then random guessing. Suspect that the models perfom equally good/bad on the training and test sets. Although the models both perform better than randomly guessing one of the outcomes, Naive Bayes does not seem like a valid model for this dataset. The fact that chaning the model complexity through alpha has no impact on model accuracy. It is no suprise that the Naive Bayes classifiers have very limited usefulness on the New York property dataset. As the models are not optimal for predicting complex datasets. It is also worth mentioning that the continous data initially are not normally distributed at all. The process of normalizing makes the data arificial. The same goes for the categorization of the sale price.**"
   ]
  },
  {
   "source": [
    "## Support Vector Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The Final ML Algorythm we will throw on the Dataset is the **Support Vector Regressor (SVR)**, Support Vector Classifyer`s regressive brother. The reason behind choosing the Support Vector Regressor over the Classifyer comes from the quriosity if it performs better than the newer and more modern Neural Netrwork Regressor we tested earlier. \n",
    "\n",
    "We will be testing traditional Support Vector Regressor with the kernels Linear and RBF, even thought there are alot of kernels, they are simply different ways off making the hyperplane decision boundary between the classes, some linearly, and some nonlinearly$^1$. Therefore we limited ourselves to one of each.\n",
    "\n",
    "Further we will test the **Linear Support Vector Regressor (LinearSVR)**, the difference between this and SVR using the Linear kernel is the epsilon parameter and tolerance (among others). This gives us more flexibility and supposedly it scales better with large numbers of samples$^2$, something that is true for us (n = 26309). \n",
    "\n",
    "Lastly we will be using the **Nu Support Vector Regression (NuSVR)**, simular to SVR, with the addition of the `nu` parameter. The `nu` parameter is a an upper boundary on the fraction of training erros and a lower bound on the fraction of support vectors$^2$. Basicly you can control the amount of support vectors used.\n",
    "\n",
    "We will first conduct a Grid Search for the three models, then we will take these results into the modeling stage, and at last discuss the outcome.\n",
    "\n",
    "In terms of preproccesing we will simply use the already scaled and handeled `standard_df` dataframe as it is prepared for a regressive Algorythm.\n",
    "\n",
    "<details>\n",
    "<summary>Sources</summary>\n",
    "[1]: Uddin, Md. Palash. (2018). Re: Diffference between SVM Linear, polynmial and RBF kernel?. Retrieved from: https://www.researchgate.net/post/Diffference_between_SVM_Linear_polynmial_and_RBF_kernel/5af811d18272c91a19463943/citation/download.\n",
    "<br>[2]: Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "</details>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 26309 entries, 0 to 26308\nData columns (total 47 columns):\n #   Column                                                               Non-Null Count  Dtype  \n---  ------                                                               --------------  -----  \n 0   NEIGHBORHOOD                                                         26309 non-null  float64\n 1   RESIDENTIAL UNITS                                                    26309 non-null  float64\n 2   COMMERCIAL UNITS                                                     26309 non-null  float64\n 3   LAND SQUARE FEET                                                     26309 non-null  float64\n 4   GROSS SQUARE FEET                                                    26309 non-null  float64\n 5   YEAR BUILT                                                           26309 non-null  float64\n 6   BUILDING CLASS AT TIME OF SALE                                       26309 non-null  float64\n 7   SALE PRICE                                                           26309 non-null  float64\n 8   MONTH SOLD                                                           26309 non-null  float64\n 9   BOROUGH_Bronx                                                        26309 non-null  float64\n 10  BOROUGH_Brooklyn                                                     26309 non-null  float64\n 11  BOROUGH_Manhattan                                                    26309 non-null  float64\n 12  BOROUGH_Queens                                                       26309 non-null  float64\n 13  BOROUGH_Staten Island                                                26309 non-null  float64\n 14  TAX CLASS AT PRESENT_Class_1                                         26309 non-null  float64\n 15  TAX CLASS AT PRESENT_Class_2                                         26309 non-null  float64\n 16  TAX CLASS AT PRESENT_Class_4                                         26309 non-null  float64\n 17  TAX CLASS AT TIME OF SALE_Class_1                                    26309 non-null  float64\n 18  TAX CLASS AT TIME OF SALE_Class_2                                    26309 non-null  float64\n 19  TAX CLASS AT TIME OF SALE_Class_4                                    26309 non-null  float64\n 20  BUILDING CLASS CATEGORY_01 ONE FAMILY DWELLINGS                      26309 non-null  float64\n 21  BUILDING CLASS CATEGORY_02 TWO FAMILY DWELLINGS                      26309 non-null  float64\n 22  BUILDING CLASS CATEGORY_03 THREE FAMILY DWELLINGS                    26309 non-null  float64\n 23  BUILDING CLASS CATEGORY_05 TAX CLASS 1 VACANT LAND                   26309 non-null  float64\n 24  BUILDING CLASS CATEGORY_06 TAX CLASS 1 - OTHER                       26309 non-null  float64\n 25  BUILDING CLASS CATEGORY_07 RENTALS - WALKUP APARTMENTS               26309 non-null  float64\n 26  BUILDING CLASS CATEGORY_08 RENTALS - ELEVATOR APARTMENTS             26309 non-null  float64\n 27  BUILDING CLASS CATEGORY_09 COOPS - WALKUP APARTMENTS                 26309 non-null  float64\n 28  BUILDING CLASS CATEGORY_10 COOPS - ELEVATOR APARTMENTS               26309 non-null  float64\n 29  BUILDING CLASS CATEGORY_14 RENTALS - 4-10 UNIT                       26309 non-null  float64\n 30  BUILDING CLASS CATEGORY_21 OFFICE BUILDINGS                          26309 non-null  float64\n 31  BUILDING CLASS CATEGORY_22 STORE BUILDINGS                           26309 non-null  float64\n 32  BUILDING CLASS CATEGORY_23 LOFT BUILDINGS                            26309 non-null  float64\n 33  BUILDING CLASS CATEGORY_26 OTHER HOTELS                              26309 non-null  float64\n 34  BUILDING CLASS CATEGORY_27 FACTORIES                                 26309 non-null  float64\n 35  BUILDING CLASS CATEGORY_29 COMMERCIAL GARAGES                        26309 non-null  float64\n 36  BUILDING CLASS CATEGORY_30 WAREHOUSES                                26309 non-null  float64\n 37  BUILDING CLASS CATEGORY_31 COMMERCIAL VACANT LAND                    26309 non-null  float64\n 38  BUILDING CLASS CATEGORY_32 HOSPITAL AND HEALTH FACILITIES            26309 non-null  float64\n 39  BUILDING CLASS CATEGORY_33 EDUCATIONAL FACILITIES                    26309 non-null  float64\n 40  BUILDING CLASS CATEGORY_35 INDOOR PUBLIC AND CULTURAL FACILITIES     26309 non-null  float64\n 41  BUILDING CLASS CATEGORY_37 RELIGIOUS FACILITIES                      26309 non-null  float64\n 42  BUILDING CLASS CATEGORY_38 ASYLUMS AND HOMES                         26309 non-null  float64\n 43  BUILDING CLASS CATEGORY_41 TAX CLASS 4 - OTHER                       26309 non-null  float64\n 44  YEAR SOLD_2016                                                       26309 non-null  float64\n 45  YEAR SOLD_2017                                                       26309 non-null  float64\n 46  k_means                                                              26309 non-null  float64\ndtypes: float64(47)\nmemory usage: 9.4 MB\n"
     ]
    }
   ],
   "source": [
    "standard_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(standard_df[\"SALE PRICE\"], dtype=float)\n",
    "x = standard_df.drop([\"SALE PRICE\"], axis = 1)\n",
    "x = np.asarray(x, dtype=float)\n",
    "\n",
    "print(\"shape of Y :\"+str(y.shape))\n",
    "print(\"shape of X :\"+str(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=.20,random_state=42)\n",
    "print(\"shape of X Train :\"+str(X_train.shape))\n",
    "print(\"shape of X Test :\"+str(X_test.shape))\n",
    "print(\"shape of Y Train :\"+str(Y_train.shape))\n",
    "print(\"shape of Y Test :\"+str(Y_test.shape))"
   ]
  },
  {
   "source": [
    "### Grid Search SVR()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modelsvr = SVR()\n",
    "param = {'kernel' : ('linear', 'rbf'),\n",
    "         'C' : [1,3,5,7,10,15]}\n",
    "\n",
    "\n",
    "grid_svr = GridSearchCV(modelsvr,param_grid = param, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_svr.fit(X_train,Y_train)\n",
    "print('Grid search with the best accuracy: \\n------------------------------------')\n",
    "print('Best parameters: ', grid_svr.best_params_) \n",
    "print('Best cross validation score(Accuracy): {:.3f}'.format(grid_svr.best_score_))\n",
    "\n",
    "print('Test set score: {:.3f}'.format(grid_svr.score(X_test,Y_test)))\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "#### Grid search with the best accuracy: \n",
    "------------------------------------\n",
    "Best parameters:  {'C': 3, 'kernel': 'rbf'}\n",
    "Best cross validation score(Accuracy): 0.700\n",
    "Test set score: 0.709"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Grid Search LinearSVR()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modelsvr = LinearSVR()\n",
    "param = {'C' : [0.001,0.01,0.1,0,2,0.3,1,3,5],\n",
    "        \"tol\": [0.00001,0.0001,0.0002,0.0004,0.001],\n",
    "        \"epsilon\": [0.0,0.3,0.5,0.7,0.9,1,1.1]}\n",
    "\n",
    "\n",
    "grid_LinSVR = GridSearchCV(modelsvr,param_grid = param, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_LinSVR.fit(X_train,Y_train)\n",
    "print('Grid search with the best accuracy: \\n------------------------------------')\n",
    "print('Best parameters: ', grid_LinSVR.best_params_) \n",
    "print('Best cross validation score(Accuracy): {:.3f}'.format(grid_LinSVR.best_score_))\n",
    "\n",
    "print('Test set score: {:.3f}'.format(grid_LinSVR.score(X_test,Y_test)))\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "#### Grid search with the best accuracy: \n",
    "------------------------------------\n",
    "Best parameters:  {'C': 0.01, 'epsilon': 0.7, 'tol': 0.0002}\n",
    "Best cross validation score(Accuracy): 0.688\n",
    "Test set score: 0.690"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Grid Search NuSVR()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modelsvr = NuSVR()\n",
    "\n",
    "param = {\"C\": [0.1,0.6,1,1.5,2],\n",
    "         \"nu\": [0.1,0.3,0.5,0.6,0.7],\n",
    "         \"kernel\": [\"linear\", \"rbf\"]}\n",
    "\n",
    "grid_NuSVR = GridSearchCV(modelsvr,param_grid = param, cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_NuSVR.fit(X_train,Y_train)\n",
    "print('Grid search with the best accuracy: \\n------------------------------------')\n",
    "print('Best parameters: ', grid_NuSVR.best_params_) \n",
    "print('Best cross validation score(Accuracy): {:.3f}'.format(grid_NuSVR.best_score_))\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "#### Grid search with the best accuracy: \n",
    "------------------------------------\n",
    "Best parameters:  {'C': 2, 'kernel': 'rbf', 'nu': 0.3}\n",
    "Best cross validation score(Accuracy): 0.703"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Fitting the models\n",
    "\n",
    "With the information we found from the grid Searches we can start fitting our models with the relevant information:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### SVR()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVR_model with RBF Kernel:\n Training score :0.746980\n Test Score: 0.708567\n"
     ]
    }
   ],
   "source": [
    "SVR_model = SVR(kernel = \"rbf\", C = 3).fit(X_train, Y_train)\n",
    "scoretrain = SVR_model.score(X_train,Y_train)\n",
    "scoretest  = SVR_model.score(X_test,Y_test)\n",
    "\n",
    "\n",
    "print(\"SVR_model with RBF Kernel:\\n Training score :{:2f}\\n Test Score: {:2f}\".format(scoretrain,scoretest))"
   ]
  },
  {
   "source": [
    "#### LinearSVR()\n",
    "\n",
    "**Note!!** \n",
    "Intrestringly enough using the cross validation function in GridSearchCV Originally gave us `C = 0.3` as the best option with `CV = 3`, running it once more using the default value (This being `CV = 5`) Proved the same results. But as shown beneath we are getting much better results with `C = 3`.\n",
    "\n",
    "We have not been able to find the reason behind this, some sources suggested issues with crossvalidating too large a part of the dataset. As of now we are using 20%, but for a later date it would be intresting to test this with different values in the test_train_split function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Support Vector Regressor with C = 3:\n Training score :0.737446\n Test Score: 0.706844\n"
     ]
    }
   ],
   "source": [
    "LinSVR_model = SVR(C = 3, epsilon = 0.7, tol = 0.0002).fit(X_train, Y_train)\n",
    "scoretrain = LinSVR_model.score(X_train,Y_train)\n",
    "scoretest  = LinSVR_model.score(X_test,Y_test)\n",
    "\n",
    "\n",
    "print(\"Linear Support Vector Regressor with C = 3:\\n Training score :{:2f}\\n Test Score: {:2f}\".format(scoretrain,scoretest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Support Vector Regressor with C = 0.3:\n Training score :0.689292\n Test Score: 0.683061\n"
     ]
    }
   ],
   "source": [
    "LinSVR_model = SVR(C = 0.3, epsilon = 0.7, tol = 0.0002).fit(X_train, Y_train)\n",
    "scoretrain = LinSVR_model.score(X_train,Y_train)\n",
    "scoretest  = LinSVR_model.score(X_test,Y_test)\n",
    "\n",
    "\n",
    "print(\"Linear Support Vector Regressor with C = 0.3:\\n Training score :{:2f}\\n Test Score: {:2f}\".format(scoretrain,scoretest))"
   ]
  },
  {
   "source": [
    "#### NuSVR()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nu Support Vector Regressor with C = 3:\n Training score :0.742859\n Test Score: 0.711971\n"
     ]
    }
   ],
   "source": [
    "NuSVR_model = NuSVR(C = 2, kernel = \"rbf\", nu = 0.3).fit(X_train, Y_train)\n",
    "scoretrain = NuSVR_model.score(X_train,Y_train)\n",
    "scoretest  = NuSVR_model.score(X_test,Y_test)\n",
    "\n",
    "\n",
    "print(\"Nu Support Vector Regressor with C = 3:\\n Training score :{:2f}\\n Test Score: {:2f}\".format(scoretrain,scoretest))"
   ]
  },
  {
   "source": [
    "### Analysing and Conclusion\n",
    "\n",
    "To start it seems that all three models deliver the same results. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd0aa6c7d94a96a0fd809867acaac96902319a9758c0a0ddb441491956cc2c3616b",
   "display_name": "Python 3.8.5  ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "aa6c7d94a96a0fd809867acaac96902319a9758c0a0ddb441491956cc2c3616b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}